
import math
import gym
from gym import spaces, logger
from gym.utils import seeding
import numpy as np

class Chopstick():
 
    def __init__(self): #변하지 않는 변수 같은 것은 이곳에 지정
        self.action_space = spaces.Discrete(9)
        self.observation_space = spaces.Box(-high, high, dtype=np.float32)

        self.seed()
        self.state = None

        self.chop = chop()
        
        self.steps_beyond_done = None
        
        self.turn = 0

    def seed(self, seed=None):
        self.np_random, seed = seeding.np_random(seed)
        return [seed]
    # 루프 게임을 수행한다
    def step(self, action, rewardsmall):
        assert self.action_space.contains(action), "%r (%s) invalid"%(action, type(action))
        state = self.state
        I_r, I_l, O_r, O_l = state
        #각각의 액션의 0~8까지의 행동 지정 
        if action == 0:
            chop.combine(self.turn, 0)
        elif action == 1:
            chop.combine(self.turn, 1)
        elif action == 2:
            chop.combine(self.turn, 2)
        elif action == 3:
            chop.combine(self.turn, 3)
        elif action == 4:
            chop.combine(self.turn, 4)
        elif action == 5:
            chop.attack(self.turn, 0, 1)
        elif action == 6:
            chop.attack(self.turn, 0, 0)
        elif action == 7:
            chop.attack(self.turn, 1, 1)
        else:
            chop.attack(self.turn, 1, 0)
      
        done =  np.sum(gamestart.hands.po[0]) == 0 \
                or np.sum(gamestart.hands.po[1]) == 0 \
        #done이 게임이 끝나면 True로 바뀐다
        done = bool(done)
        # 안끝나면 둘다 보상 없음 끝나고 내가 이기면 검정이 보상 1 받음 상대가 이기면 보상 -1
        if not done:
            reward = 0
        else:
            if I_win: #
                reward = 100
            else:
                reward = -100

        return np.array(self.state), reward + rewardsmall, done, {}

    #게임을 리셋
    def reset(self):
        self.state = np.ones((2,2)) #원래 상태를 이렇게 돌려놓아 준건데 내가 잘못 넣은 거면 수정 부탁
        return np.array(self.state)


    def close(self):
        if self.viewer:
            self.viewer.close()
            self.viewer = None
            

#############################################################################################################################

if True:
    #  환경 지정, 최대 타임스텝 수가 몇갠지는 위에 episode로
    env = gym.make('Chopstick')
    state_size = (2,2)
    action_size = 9


    # DQN 에이전트 생성
    agent = DQNAgent(state_size, action_size)

    victory, episodes = [], []

    for e in range(EPISODES):
        done = False
        # env 초기화
        state = env.reset()
        state = np.reshape(state, [1, state_size])

        while not done:
            if agent.render:
                env.render()

                
            # 현재 상태로 행동을 선택
            rewardsmall = 0
            while True:
                action = agent.get_action(state)
                if action <= 4:
                    if action > np.sum(env.chop.po[env.turn]) or action == env.chop.po[env.turn][1] or action == env.chop.po[env.turn][0]:
                        rewardsmall -= 1
                        continue
                else:
                    if env.chop.po[env.turn][action // 7] <= 0 or env.chop.po[1 - env.turn][action % 2] <= 0:
                        rewardsmall -= 1
                        continue
                
                break
            
            
            # 선택한 행동으로 환경에서 한 타임스텝 진행
            next_state, reward, done, info = env.step(action, rewardsmall)
            next_state = np.reshape(next_state, [1, state_size])
            # 에피소드가 중간에 끝나면 -100 보상
            reward = reward if not done or score == 499 else -100

            # 리플레이 메모리에 샘플 <s, a, r, s'> 저장
            agent.append_sample(state, action, reward, next_state, done)
            # 매 타임스텝마다 학습
            if len(agent.memory) >= agent.train_start:
                agent.train_model()

            
            state = next_state

            if done:
                # 각 에피소드마다 타깃 모델을 모델의 가중치로 업데이트
                agent.update_target_model()
                
                # 이기면 victory가 1 올라가고 아니면 그대로 유지
                if I_win:
                    victory = 1
                else :
                    victory = 0
                   
                    
                # 에피소드마다 학습 결과 출력
                victory.append(victory)
                episodes.append(e)
                pylab.plot(episodes, scores, 'b')
                pylab.savefig("./save_graph/cartpole_dqn.png")
                print("episode:", e, "  victroy:", victory , "total_victory:" , sum(victory) , "  memory length:",
                      len(agent.memory), "  epsilon:", agent.epsilon)
            env.turn = 1 - env.turn
